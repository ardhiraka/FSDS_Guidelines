{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"colab":{"name":"d2pm.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"x57tHTUyuZoC"},"source":["# Week 1: Day 2 PM // Training Deep Neural Networks Pt.2\n"]},{"cell_type":"markdown","metadata":{"id":"XvjbyImEObZI"},"source":["Today we will learn about:\n","\n","\n","1. Reusing Pretrained Layers\n","2. Optimizers\n","\n"]},{"cell_type":"code","metadata":{"id":"yoZXtk-qvrlG"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","# Common imports\n","import numpy as np\n","import os\n","\n","# to make this notebook's output stable across runs\n","np.random.seed(42)\n","\n","# To plot pretty figures\n","%matplotlib inline\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fabrEzItvofT"},"source":["## Load Dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y1kjwQ25vqGI","executionInfo":{"status":"ok","timestamp":1622688227799,"user_tz":-420,"elapsed":304,"user":{"displayName":"Muhammad Adi Nugroho","photoUrl":"","userId":"16949655937383207580"}},"outputId":"1f04f91c-820e-4fd9-ecf9-bbfe0120c588"},"source":["(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n","X_train_full = X_train_full / 255.0\n","X_test = X_test / 255.0\n","X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n","y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n","8192/5148 [===============================================] - 0s 0us/step\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UGCvRD25OscR"},"source":["# Reusing Pretrained Layers"]},{"cell_type":"markdown","metadata":{"id":"l0XB8Ob6PM0V"},"source":["\n","Let's split the fashion MNIST training set in two:\n","\n"," \n","\n","*   X_train_A: all images of all items except for sandals and shirts (classes 5 and 6).\n","*   X_train_B: a much smaller training set of just the first 200 images of sandals or shirts.\n","\n","\n","X_train_B: a much smaller training set of just the first 200 images of sandals or shirts.\n","The validation set and the test set are also split this way, but without restricting the number of images.\n","\n","We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification). We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). However, since we are using Dense layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image, as we will see in the CNN chapter)."]},{"cell_type":"markdown","metadata":{"id":"ztAjxnO3Qwl5"},"source":["### Train Model A"]},{"cell_type":"code","metadata":{"id":"ggmBJkFwQoqW"},"source":["def split_dataset(X, y):\n","    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n","    y_A = y[~y_5_or_6]\n","    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n","    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n","    return ((X[~y_5_or_6], y_A),\n","            (X[y_5_or_6], y_B))\n","\n","(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n","(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n","(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n","X_train_B = X_train_B[:200]\n","y_train_B = y_train_B[:200]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fMCzdSf5QsuP"},"source":["model_A = keras.models.Sequential()\n","model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n","for n_hidden in (300, 100, 50, 50, 50):\n","    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n","model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l8cWGGH5QuWM","executionInfo":{"status":"ok","timestamp":1622688573768,"user_tz":-420,"elapsed":476,"user":{"displayName":"Muhammad Adi Nugroho","photoUrl":"","userId":"16949655937383207580"}},"outputId":"b9f11049-e0d5-4ac8-f1f1-89058b9cb3b1"},"source":["model_A.compile(loss=\"sparse_categorical_crossentropy\",\n","                optimizer=keras.optimizers.SGD(lr=1e-3),\n","                metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YQmyTRiqQv3A","executionInfo":{"status":"ok","timestamp":1622688647261,"user_tz":-420,"elapsed":72186,"user":{"displayName":"Muhammad Adi Nugroho","photoUrl":"","userId":"16949655937383207580"}},"outputId":"61b474b7-7098-48e8-fb55-3dcc8b5dae5d"},"source":["history = model_A.fit(X_train_A, y_train_A, epochs=20,\n","                    validation_data=(X_valid_A, y_valid_A))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","1375/1375 [==============================] - 4s 3ms/step - loss: 0.6476 - accuracy: 0.7927 - val_loss: 0.4018 - val_accuracy: 0.8670\n","Epoch 2/20\n","1375/1375 [==============================] - 3s 2ms/step - loss: 0.3708 - accuracy: 0.8754 - val_loss: 0.3356 - val_accuracy: 0.8832\n","Epoch 3/20\n","1375/1375 [==============================] - 3s 3ms/step - loss: 0.3273 - accuracy: 0.8886 - val_loss: 0.3187 - val_accuracy: 0.8909\n","Epoch 4/20\n","1375/1375 [==============================] - 4s 3ms/step - loss: 0.3054 - accuracy: 0.8954 - val_loss: 0.2938 - val_accuracy: 0.9016\n","Epoch 5/20\n","1375/1375 [==============================] - 4s 3ms/step - loss: 0.2912 - accuracy: 0.9009 - val_loss: 0.2822 - val_accuracy: 0.9048\n","Epoch 6/20\n","1375/1375 [==============================] - 4s 3ms/step - loss: 0.2802 - accuracy: 0.9052 - val_loss: 0.2733 - val_accuracy: 0.9078\n","Epoch 7/20\n","1375/1375 [==============================] - 4s 3ms/step - loss: 0.2715 - accuracy: 0.9073 - val_loss: 0.2735 - val_accuracy: 0.9051\n","Epoch 8/20\n","1375/1375 [==============================] - 4s 3ms/step - loss: 0.2643 - accuracy: 0.9103 - val_loss: 0.2623 - val_accuracy: 0.9098\n","Epoch 9/20\n","1375/1375 [==============================] - 4s 3ms/step - loss: 0.2579 - accuracy: 0.9131 - val_loss: 0.2693 - val_accuracy: 0.9108\n","Epoch 10/20\n","1375/1375 [==============================] - 3s 2ms/step - loss: 0.2527 - accuracy: 0.9144 - val_loss: 0.2546 - val_accuracy: 0.9145\n","Epoch 11/20\n","1375/1375 [==============================] - 4s 3ms/step - loss: 0.2483 - accuracy: 0.9156 - val_loss: 0.2529 - val_accuracy: 0.9136\n","Epoch 12/20\n","1375/1375 [==============================] - 4s 3ms/step - loss: 0.2441 - accuracy: 0.9165 - val_loss: 0.2509 - val_accuracy: 0.9155\n","Epoch 13/20\n","1375/1375 [==============================] - 4s 3ms/step - loss: 0.2400 - accuracy: 0.9182 - val_loss: 0.2482 - val_accuracy: 0.9138\n","Epoch 14/20\n","1375/1375 [==============================] - 4s 3ms/step - loss: 0.2367 - accuracy: 0.9195 - val_loss: 0.2475 - val_accuracy: 0.9158\n","Epoch 15/20\n","1375/1375 [==============================] - 3s 3ms/step - loss: 0.2332 - accuracy: 0.9200 - val_loss: 0.2513 - val_accuracy: 0.9096\n","Epoch 16/20\n","1375/1375 [==============================] - 4s 3ms/step - loss: 0.2305 - accuracy: 0.9218 - val_loss: 0.2403 - val_accuracy: 0.9203\n","Epoch 17/20\n","1375/1375 [==============================] - 4s 3ms/step - loss: 0.2279 - accuracy: 0.9223 - val_loss: 0.2410 - val_accuracy: 0.9183\n","Epoch 18/20\n","1375/1375 [==============================] - 4s 3ms/step - loss: 0.2251 - accuracy: 0.9229 - val_loss: 0.2377 - val_accuracy: 0.9205\n","Epoch 19/20\n","1375/1375 [==============================] - 4s 3ms/step - loss: 0.2225 - accuracy: 0.9233 - val_loss: 0.2388 - val_accuracy: 0.9200\n","Epoch 20/20\n","1375/1375 [==============================] - 4s 3ms/step - loss: 0.2200 - accuracy: 0.9239 - val_loss: 0.2343 - val_accuracy: 0.9218\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"I7ecE7x8iAvI"},"source":["model_A.save(\"my_model_A.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hgX_q0vLR8JO"},"source":["### Train Model B"]},{"cell_type":"markdown","metadata":{"id":"Msbn2QetR-Su"},"source":["Let's try to train Model B from scratch"]},{"cell_type":"code","metadata":{"id":"n-kXuI_UR9sn"},"source":["model_B = keras.models.Sequential()\n","model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n","for n_hidden in (300, 100, 50, 50, 50):\n","    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n","model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dWIApeOPSHCR"},"source":["model_B.compile(loss=\"binary_crossentropy\",\n","                optimizer=keras.optimizers.SGD(lr=1e-3),\n","                metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FZKCuqZpSIni"},"source":["history = model_B.fit(X_train_B, y_train_B, epochs=20,\n","                      validation_data=(X_valid_B, y_valid_B))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zFbHiRUvRXt7"},"source":["### Train Model B with Model A"]},{"cell_type":"code","metadata":{"id":"tRP3pE78RmWd"},"source":["model_A = keras.models.load_model(\"my_model_A.h5\")\n","model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n","model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gsHWYfUYRoyV"},"source":["model_A_clone = keras.models.clone_model(model_A)\n","model_A_clone.set_weights(model_A.get_weights())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4my3IreaRq4L"},"source":["for layer in model_B_on_A.layers[:-1]:\n","    layer.trainable = False\n","\n","model_B_on_A.compile(loss=\"binary_crossentropy\",\n","                     optimizer=keras.optimizers.SGD(lr=1e-3),\n","                     metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H1f9br2nRzCs"},"source":["history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n","                           validation_data=(X_valid_B, y_valid_B))\n","\n","for layer in model_B_on_A.layers[:-1]:\n","    layer.trainable = True\n","\n","model_B_on_A.compile(loss=\"binary_crossentropy\",\n","                     optimizer=keras.optimizers.SGD(lr=1e-3),\n","                     metrics=[\"accuracy\"])\n","history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n","                           validation_data=(X_valid_B, y_valid_B))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3vvbuY5Tv-Qk"},"source":["# Optimizers"]},{"cell_type":"markdown","metadata":{"id":"546yjzNF4lx0"},"source":["In neural network, there are many methods to find the best parameter, but the most popular is gradient descent. Remember that in NN our objective is to minimize a cost function, thus we hypotheses that by changing the weights following the gradient of cost function with regard to the weights, we can eventually achieve weights with lowest cost value. \n","\n","In general, weights update can be expressed by equation. Keep in mind the implicit negative notation can be switched, either included in the update term ($\\Delta_{t}$) or outside of it.\n","\n","$\\theta_{t+1}=\\theta_{t}+\\Delta \\theta_{t}$"]},{"cell_type":"markdown","metadata":{"id":"_WpPTiyxwDEI"},"source":["The standard stochastic gradient descent, usually refered as SGD, is the simplest form of gradient descent. The only parameter of this gradient descent is its learning rate.\n","\n","$\\Delta \\theta_{t}=-\\eta\\frac{\\delta J}{\\delta{\\theta}}$\n"]},{"cell_type":"code","metadata":{"id":"pUL2_OmgwP4O"},"source":["optim_sgd=tf.keras.optimizers.SGD(\n","    learning_rate=0.01, momentum=0.0, nesterov=False, name=\"SGD\", **kwargs\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UvCx22jWwQLb"},"source":["SGD with Momentum: Momentum term help stabilizes the update by including a short term memory. Mathematically, this is done by including the value of previous update to current update calculation. The parameter now become 2, one is the learning rate and the other is the momentum term.\n","\n","$\\Delta \\theta_{t}=\\mu\\Delta \\theta_{t-1}-\\eta\\frac{\\delta J}{\\delta{\\theta}}$ "]},{"cell_type":"code","metadata":{"id":"e9Z2RgWEwh_s"},"source":["optim_sgd_mom = tf.keras.optimizers.SGD(\n","    learning_rate=0.01, momentum=0.9, nesterov=False, name=\"SGD_Momentum\", **kwargs\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HXqr8RqgwiOg"},"source":["Nesterov Momentum: Nesterov Momentum adjust the direction of the regular momentum update."]},{"cell_type":"markdown","metadata":{"id":"AxGVE0iN6imb"},"source":["$\\Delta \\theta_{t}=\\mu\\Delta \\theta_{t-1}-\\eta\\frac{\\delta J}{\\delta{\\theta}}(\\theta_{t}+\\mu\\Delta_{t-1})$ "]},{"cell_type":"code","metadata":{"id":"FpKI-wKLwnlS"},"source":["optim_sgd_ntv = tf.keras.optimizers.SGD(\n","    learning_rate=0.01, momentum=0.9, nesterov=True, name=\"SGD_Nesterov\", **kwargs\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1K2BzNbrB-TZ"},"source":["Adagrad is an optimizer with parameter-specific learning rates, which are adapted relative to how frequently a parameter gets updated during training. The more updates a parameter receives, the smaller the updates.\n","\n","$v_t=v_{t-1}+(\\frac{\\delta J}{\\delta{\\theta}})^{2} $\n","\n","$\\Delta \\theta_{t}=-\\frac{\\eta}{\\sqrt{v_t+\\epsilon}}(\\frac{\\delta J}{\\delta{\\theta}})$\n","\n","Adagrad essentialy only need one parameter, which is the initial learning rate $\\eta$. But, on practice we can set small value $\\epsilon$ to avoid division by zero."]},{"cell_type":"code","metadata":{"id":"YVDWNlrdB-LK"},"source":["optim_adagrad = tf.keras.optimizers.Adagrad(\n","    learning_rate=1,\n","    epsilon=1e-07,\n","    name=\"Adagrad\",\n","    **kwargs\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YvH0ahL3wnA4"},"source":["RMSProp use moving average of squared gradients to scale the weight update. It is proposed by Geoffrey Hinton (He is one of the \"godfathers\" of deep learning) to solve the problem of diminishing learning rates on AdaGrad. For RMSProp we use moving average, instead of keeping all the average like what AdaGrad did.\n","Expressing moving average of squared gradients as $v$, we can calculate the weight update as follows:\n","\n","$v_t=\\rho v_{t-1}+(1-\\rho)(\\frac{\\delta J}{\\delta{\\theta}})^{2} $\n","\n","$\\Delta \\theta_{t}=-\\frac{\\eta}{\\sqrt{v_t+\\epsilon}}(\\frac{\\delta J}{\\delta{\\theta}})$ \n","\n","There are two essential parameters, $\\rho$ the discounting factor and $\\eta$ the learning rate. Again, for numerical stability we add small $\\epsilon$"]},{"cell_type":"code","metadata":{"id":"UFYtfh-QCCul"},"source":["optim_rmsprop = tf.keras.optimizers.RMSprop(\n","    learning_rate=0.001,\n","    rho=0.9,\n","    epsilon=1e-07,\n","    name=\"RMSprop\"\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KSRalKuttPPE"},"source":["Adadelta optimization is a stochastic gradient descent method that is based on adaptive learning rate per dimension to address two drawbacks:\n","The continual decay of learning rates throughout training.\n","The need for a manually selected global learning rate.\n","Similar to RMSProp, AdaDelta also try to solve the problem of RMSProp. It is developed independently, thus they are somehow similar. AdaDelta use another exponentially decaying average, this time not of squared gradients but of squared parameter updates.\n","\n","$v_t=\\rho v_{t-1}+(1-\\rho)(\\frac{\\delta J}{\\delta{\\theta}})^{2} $\n","\n","$x_t=\\rho x_{t-1}+(1-\\rho)(\\Delta \\theta_{t})^{2} $\n","\n","$\\Delta \\theta_{t}=-\\frac{\\eta\\sqrt{x_t+\\epsilon}}{\\sqrt{v_t+\\epsilon}}(\\frac{\\delta J}{\\delta{\\theta}})$ \n","\n","On the original paper AdaDelta do not use any learning rate parameter, but in Keras we can set it just as in other method and as written on the equation."]},{"cell_type":"code","metadata":{"id":"RjVemNh_tT_U"},"source":["optim_adadelta = tf.keras.optimizers.AdaDelta(\n","    learning_rate=0.001,\n","    rho=0.9,\n","    epsilon=1e-07,\n","    name=\"RMSprop\"\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LzkXRuZotURd"},"source":["Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.ADAM is just Adadelta (which rescales gradients based on accumulated \"second-order\" information) plus momentum (which smooths gradients based on accumulated \"first-order\" information). \n","\n","$s_t=\\beta_{1} v_{t-1}+(1-\\beta_{1})(\\frac{\\delta J}{\\delta{\\theta}}) $\n","\n","$v_t=\\beta_{2} x_{t-1}+(1-\\beta_{2})(\\frac{\\delta J}{\\delta{\\theta}})^{2} $\n","\n","$\\Delta \\theta_{t}=-\\frac{\\eta s_t}{\\sqrt{v_t+\\epsilon}}(\\frac{\\delta J}{\\delta{\\theta}})$ \n"]},{"cell_type":"code","metadata":{"id":"Z43Rp8ciYMg5"},"source":["optim_adam = tf.keras.optimizers.Adam(\n","    learning_rate=0.001,\n","    rho=0.9,\n","    epsilon=1e-07,\n","    name=\"RMSprop\"\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_sPkfXhXiroo"},"source":["Generally, we prefer the adaptive learning rate or momentum method as the more epoch the learning has through, we need smaller learning rate to avoid fluctuation of the weights. However, there are some cases where the Nesterov optimizer give better performance when we hit the right parameter. "]}]}